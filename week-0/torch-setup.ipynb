{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b612e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DirectML Device: privateuseone:0\n",
      "Successfully connected to GPU!\n",
      "Result of tensor addition: tensor([5., 7., 9.], device='privateuseone:0')\n",
      "Tensor is on device: privateuseone:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "\n",
    "# 1. Initialize the DirectML device\n",
    "device = torch_directml.device()\n",
    "print(f\"DirectML Device: {device}\")\n",
    "\n",
    "# 2. Check if the device is actually available\n",
    "# (DirectML usually defaults to CPU if no compatible GPU is found)\n",
    "try:\n",
    "  # Create a tensor directly on the DirectML device\n",
    "  x = torch.tensor([1.0, 2.0, 3.0]).to(device)\n",
    "  y = torch.tensor([4.0, 5.0, 6.0]).to(device)\n",
    "\n",
    "  # Perform a simple calculation\n",
    "  z = x + y\n",
    "\n",
    "  print(\"Successfully connected to GPU!\")\n",
    "  print(f\"Result of tensor addition: {z}\")\n",
    "  print(f\"Tensor is on device: {z.device}\")\n",
    "\n",
    "except Exception as e:\n",
    "  print(\"Could not access the GPU via DirectML.\")\n",
    "  print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59acac11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Size: 8192x8192\n",
      "Warming up GPU...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 38.4022,  25.3507,  19.4983,  ...,  22.6373,  -8.0272, -38.5622],\n",
       "        [ 31.2668,   7.4328,  40.5782,  ...,   7.9537,  18.3317, -25.9368],\n",
       "        [ -9.6734,  16.4145,  -0.2959,  ...,  -4.6618,  69.1028,  -2.6722],\n",
       "        ...,\n",
       "        [-37.9909,  27.2076,  16.3519,  ...,  35.0770,   1.7056,   3.3922],\n",
       "        [ -4.0383,  14.5772,  -3.2566,  ...,  18.1526, -34.8598,   1.3983],\n",
       "        [ -1.3562,  54.9273,  34.9020,  ...,  15.0746,  21.2836,  17.9908]],\n",
       "       device='privateuseone:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing GPU (privateuseone:0)...\n",
      "--- Preparing Data on privateuseone:0 ---\n",
      "--- Started Computation on privateuseone:0 ---\n",
      "GPU Time: 0.4937 seconds\n",
      "\n",
      "Testing CPU...\n",
      "--- Preparing Data on cpu ---\n",
      "--- Started Computation on cpu ---\n",
      "CPU Time: 5.2707 seconds\n",
      "\n",
      "==============================\n",
      "RESULT: GPU is 10.68x faster than CPU\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Size of the square matrix (N x N).\n",
    "# 4096 is ~100MB per tensor. 8192 is ~400MB.\n",
    "# Adjust 'MATRIX_SIZE' based on your RAM/VRAM.\n",
    "MATRIX_SIZE = 8192\n",
    "\n",
    "\n",
    "def get_device():\n",
    "  if torch_directml.is_available():\n",
    "    return torch_directml.device()\n",
    "  return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def stress_test(device, size):\n",
    "  print(f\"--- Preparing Data on {device} ---\")\n",
    "  # Generate random matrices\n",
    "  # We use float32 (standard precision)\n",
    "  x = torch.randn(size, size, device=device)\n",
    "  y = torch.randn(size, size, device=device)\n",
    "\n",
    "  print(f\"--- Started Computation on {device} ---\")\n",
    "  start_time = time.perf_counter()\n",
    "\n",
    "  # The heavy operation: Matrix Multiplication\n",
    "  z = torch.mm(x, y)\n",
    "\n",
    "  # FORCE SYNCHRONIZATION\n",
    "  # We pull one value to CPU to ensure the GPU has actually finished\n",
    "  # the calculation before we stop the clock.\n",
    "  _ = z.min().item()\n",
    "\n",
    "  end_time = time.perf_counter()\n",
    "  return end_time - start_time\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  dml_device = get_device()\n",
    "  cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "  print(f\"Matrix Size: {MATRIX_SIZE}x{MATRIX_SIZE}\")\n",
    "  print(\"Warming up GPU...\")\n",
    "  # Warm up to remove initialization overhead\n",
    "  warmup = torch.randn(1024, 1024, device=dml_device)\n",
    "  torch.mm(warmup, warmup)\n",
    "\n",
    "  # 1. Test GPU\n",
    "  print(f\"\\nTesting GPU ({dml_device})...\")\n",
    "  try:\n",
    "    gpu_time = stress_test(dml_device, MATRIX_SIZE)\n",
    "    print(f\"GPU Time: {gpu_time:.4f} seconds\")\n",
    "  except Exception as e:\n",
    "    print(f\"GPU Failed: {e}\")\n",
    "    gpu_time = None\n",
    "\n",
    "  # 2. Test CPU\n",
    "  print(f\"\\nTesting CPU...\")\n",
    "  try:\n",
    "    cpu_time = stress_test(cpu_device, MATRIX_SIZE)\n",
    "    print(f\"CPU Time: {cpu_time:.4f} seconds\")\n",
    "  except KeyboardInterrupt:\n",
    "    print(\"CPU test stopped manually (it was taking too long!)\")\n",
    "    cpu_time = None\n",
    "\n",
    "  # 3. Results\n",
    "  if gpu_time and cpu_time:\n",
    "    speedup = cpu_time / gpu_time\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"RESULT: GPU is {speedup:.2f}x faster than CPU\")\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c858a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading/Loading MNIST Data...\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:07<00:00, 1357198.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 125415.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 1073828.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 4507933.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Data Ready.\n",
      "\n",
      "\n",
      "--- Starting Training on GPU (DIRECTML) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\k26ra\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:534: UserWarning: The operator 'aten::lerp.Scalar_out' is not currently supported on the DML backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at C:\\__w\\1\\s\\pytorch-directml-plugin\\torch_directml\\csrc\\dml\\dml_cpu_fallback.cpp:17.)\n",
      "  torch._foreach_lerp_(device_exp_avgs, device_grads, 1 - beta1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 complete. Avg Loss: 0.1879\n",
      "Epoch 2/5 complete. Avg Loss: 0.0492\n",
      "Epoch 3/5 complete. Avg Loss: 0.0352\n",
      "Epoch 4/5 complete. Avg Loss: 0.0265\n",
      "Epoch 5/5 complete. Avg Loss: 0.0190\n",
      "--> GPU (DirectML) finished in 129.7837 seconds\n",
      "\n",
      "--- Starting Training on CPU ---\n",
      "Epoch 1/5 complete. Avg Loss: 0.1510\n",
      "Epoch 2/5 complete. Avg Loss: 0.0471\n",
      "Epoch 3/5 complete. Avg Loss: 0.0318\n",
      "Epoch 4/5 complete. Avg Loss: 0.0227\n",
      "Epoch 5/5 complete. Avg Loss: 0.0165\n",
      "--> CPU finished in 241.7296 seconds\n",
      "\n",
      "========================================\n",
      "FINAL RESULT:\n",
      "GPU Time: 129.78s\n",
      "CPU Time: 241.73s\n",
      "Speedup:  1.86x FASTER on GPU\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_directml\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "BATCH_SIZE = 128    # Higher batch size uses more VRAM\n",
    "EPOCHS = 5          # Number of times to loop through the ENTIRE dataset\n",
    "# (MNIST is 60,000 images, so 5 epochs = 300,000 image passes)\n",
    "\n",
    "# --- THE NEURAL NETWORK ---\n",
    "\n",
    "\n",
    "class StressCNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(StressCNN, self).__init__()\n",
    "    # A standard CNN architecture\n",
    "    self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "    self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "    self.fc2 = nn.Linear(128, 10)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.pool(self.relu(self.conv1(x)))\n",
    "    x = self.pool(self.relu(self.conv2(x)))\n",
    "    x = x.view(-1, 64 * 7 * 7)  # Flatten\n",
    "    x = self.relu(self.fc1(x))\n",
    "    x = self.fc2(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def train_model(device_name, device_obj, dataloader):\n",
    "  print(f\"\\n--- Starting Training on {device_name.upper()} ---\")\n",
    "\n",
    "  model = StressCNN().to(device_obj)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "  model.train()  # Set to training mode\n",
    "\n",
    "  start_time = time.perf_counter()\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    # Iterate over batches\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "      # 1. Move data to device (CRITICAL STEP FOR SPEED)\n",
    "      images, labels = images.to(device_obj), labels.to(device_obj)\n",
    "\n",
    "      # 2. Zero gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # 3. Forward pass\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      # 4. Backward pass (Heavy computation)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} complete. Avg Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "  # Force synchronization for DirectML/GPU to ensure timing is accurate\n",
    "  if \"cpu\" not in str(device_obj):\n",
    "    # We perform a small read to force the GPU to finish all pending work\n",
    "    _ = torch.tensor([1]).to(device_obj).cpu()\n",
    "\n",
    "  end_time = time.perf_counter()\n",
    "  duration = end_time - start_time\n",
    "  print(f\"--> {device_name} finished in {duration:.4f} seconds\")\n",
    "  return duration\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  # 1. Setup Data\n",
    "  print(\"Downloading/Loading MNIST Data...\")\n",
    "  transform = transforms.Compose([\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize((0.1307,), (0.3081,))\n",
    "  ])\n",
    "  dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "  dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "  print(\"Data Ready.\\n\")\n",
    "\n",
    "  # 2. Define Devices\n",
    "  dml_device = torch_directml.device() if torch_directml.is_available() else None\n",
    "  cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "  # 3. Run Benchmarks\n",
    "  gpu_time = 0\n",
    "  if dml_device:\n",
    "    # We run GPU first\n",
    "    gpu_time = train_model(\"GPU (DirectML)\", dml_device, dataloader)\n",
    "  else:\n",
    "    print(\"DirectML not found. Skipping GPU test.\")\n",
    "\n",
    "  # Run CPU\n",
    "  # WARNING: CPU training on CNNs is notoriously slow.\n",
    "  cpu_time = train_model(\"CPU\", cpu_device, dataloader)\n",
    "\n",
    "  # 4. Comparison\n",
    "  if gpu_time > 0:\n",
    "    speedup = cpu_time / gpu_time\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"FINAL RESULT:\")\n",
    "    print(f\"GPU Time: {gpu_time:.2f}s\")\n",
    "    print(f\"CPU Time: {cpu_time:.2f}s\")\n",
    "    print(f\"Speedup:  {speedup:.2f}x FASTER on GPU\")\n",
    "    print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad2209c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Optimization Stress Test on privateuseone:0 ---\n",
      "\n",
      "Testing Adam (uses lerp -> CPU Fallback)...\n",
      "-> Time: 8.1624 seconds\n",
      "\n",
      "Testing SGD (Pure GPU)...\n",
      "-> Time: 4.7863 seconds\n",
      "\n",
      "========================================\n",
      "SGD is 1.71x faster than Adam on DirectML!\n",
      "Recommendation: Use SGD with Momentum for now.\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_directml\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Suppress the warning so we can see the timing results clearly\n",
    "warnings.filterwarnings(\"ignore\", message=\".*aten::lerp.Scalar_out.*\")\n",
    "\n",
    "# Setup\n",
    "dml = torch_directml.device()\n",
    "print(f\"--- Optimization Stress Test on {dml} ---\\n\")\n",
    "\n",
    "# A dummy model and data\n",
    "model = nn.Linear(4096, 4096).to(dml)\n",
    "data = torch.randn(1024, 4096, device=dml)\n",
    "target = torch.randn(1024, 4096, device=dml)\n",
    "\n",
    "\n",
    "def test_optimizer(opt_class, name, **kwargs):\n",
    "  # Reset model parameters to ensure fair test\n",
    "  model.reset_parameters()\n",
    "  optimizer = opt_class(model.parameters(), **kwargs)\n",
    "  criterion = nn.MSELoss()\n",
    "\n",
    "  print(f\"Testing {name}...\")\n",
    "  start = time.perf_counter()\n",
    "\n",
    "  # Run 50 optimization steps\n",
    "  for _ in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()  # <--- This is where Adam hits the CPU wall\n",
    "\n",
    "  # Sync GPU\n",
    "  _ = torch.tensor([1]).to(dml).cpu()\n",
    "\n",
    "  duration = time.perf_counter() - start\n",
    "  print(f\"-> Time: {duration:.4f} seconds\\n\")\n",
    "  return duration\n",
    "\n",
    "\n",
    "# 1. Test Adam (Will likely trigger hidden CPU fallbacks)\n",
    "adam_time = test_optimizer(optim.Adam, \"Adam (uses lerp -> CPU Fallback)\", lr=0.001)\n",
    "\n",
    "# 2. Test SGD (Should stay 100% on GPU)\n",
    "sgd_time = test_optimizer(optim.SGD, \"SGD (Pure GPU)\", lr=0.01)\n",
    "\n",
    "print(\"=\"*40)\n",
    "if adam_time > sgd_time:\n",
    "  print(f\"SGD is {adam_time / sgd_time:.2f}x faster than Adam on DirectML!\")\n",
    "  print(\"Recommendation: Use SGD with Momentum for now.\")\n",
    "else:\n",
    "  print(\"Speeds are similar (Fallback might be negligible for this model size).\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f409d7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup: Batch Size=64 | Epochs=3\n",
      "\n",
      "--- Starting Benchmark on GPU (DirectML) ---\n",
      "Epoch 1/3 | Avg Loss: 0.3042\n",
      "Epoch 2/3 | Avg Loss: 0.0595\n",
      "Epoch 3/3 | Avg Loss: 0.0399\n",
      "--> GPU (DirectML) Time: 51.5093 seconds\n",
      "\n",
      "--- Starting Benchmark on CPU ---\n",
      "Epoch 1/3 | Avg Loss: 0.3124\n",
      "Epoch 2/3 | Avg Loss: 0.0583\n",
      "Epoch 3/3 | Avg Loss: 0.0381\n",
      "--> CPU Time: 496.0633 seconds\n",
      "\n",
      "========================================\n",
      "FINAL RESULTS (Batch 64):\n",
      "GPU Time: 51.51s\n",
      "CPU Time: 496.06s\n",
      "Speedup:  9.63x FASTER on GPU\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_directml\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# 64 is very safe for 4GB VRAM.\n",
    "# It creates more \"overhead\" (more CPU-GPU chatter), but it won't crash.\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 3       # Reduced to 3 to get quick results\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class HeavyCNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(HeavyCNN, self).__init__()\n",
    "    # We keep the model heavy to force computation\n",
    "    self.features = nn.Sequential(\n",
    "        nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(256 * 7 * 7, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 10)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.features(x)\n",
    "    x = self.classifier(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def run_benchmark(device_name, device_obj, dataloader):\n",
    "  print(f\"\\n--- Starting Benchmark on {device_name} ---\")\n",
    "\n",
    "  # CLEAR MEMORY FIRST\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "  try:\n",
    "    model = HeavyCNN().to(device_obj)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Warmup\n",
    "    print(\"Warming up...\", end=\"\\r\")\n",
    "    dummy = torch.randn(BATCH_SIZE, 1, 28, 28).to(device_obj)\n",
    "    _ = model(dummy)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "      running_loss = 0.0\n",
    "      for i, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device_obj, non_blocking=True)\n",
    "        labels = labels.to(device_obj, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "      # Print progress every epoch so you know it's not frozen\n",
    "      print(f\"Epoch {epoch+1}/{EPOCHS} | Avg Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    # Sync\n",
    "    if \"cpu\" not in str(device_obj):\n",
    "      _ = torch.tensor([1]).to(device_obj).cpu()\n",
    "\n",
    "    duration = time.perf_counter() - start_time\n",
    "    print(f\"--> {device_name} Time: {duration:.4f} seconds\")\n",
    "    return duration\n",
    "\n",
    "  except RuntimeError as e:\n",
    "    print(f\"\\nCRITICAL ERROR on {device_name}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  print(f\"Setup: Batch Size={BATCH_SIZE} | Epochs={EPOCHS}\")\n",
    "\n",
    "  # Data Setup\n",
    "  transform = transforms.ToTensor()\n",
    "  train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "\n",
    "  # GPU Loader\n",
    "  gpu_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          pin_memory=True, num_workers=2)\n",
    "\n",
    "  # CPU Loader\n",
    "  cpu_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          pin_memory=False, num_workers=0)\n",
    "\n",
    "  dml = torch_directml.device()\n",
    "  cpu = torch.device(\"cpu\")\n",
    "\n",
    "  # 1. GPU Test\n",
    "  gpu_time = run_benchmark(\"GPU (DirectML)\", dml, gpu_loader)\n",
    "\n",
    "  # 2. CPU Test\n",
    "  if gpu_time:\n",
    "    cpu_time = run_benchmark(\"CPU\", cpu, cpu_loader)\n",
    "\n",
    "    # 3. Final Report\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"FINAL RESULTS (Batch {BATCH_SIZE}):\")\n",
    "    print(f\"GPU Time: {gpu_time:.2f}s\")\n",
    "    print(f\"CPU Time: {cpu_time:.2f}s\")\n",
    "    speedup = cpu_time / gpu_time\n",
    "    print(f\"Speedup:  {speedup:.2f}x FASTER on GPU\")\n",
    "    print(\"=\"*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
